#!/usr/bin/env python3
"""
Samay v3 - Response Aggregator
==============================
Advanced response aggregation and analysis for multi-agent queries
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass
from pathlib import Path

from .prompt_dispatcher import AggregatedResponse, ServiceResponse


@dataclass
class ComparisonAnalysis:
    """Analysis comparing responses across services"""
    similarity_score: float  # 0-1 scale
    common_themes: List[str]
    unique_insights: Dict[str, List[str]]  # service -> unique points
    consensus_answer: str
    conflicting_points: List[str]


class ResponseAggregator:
    """Advanced aggregation and analysis of multi-agent responses"""
    
    def __init__(self, output_dir: str = "reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def create_comprehensive_report(self, response: AggregatedResponse) -> str:
        """
        Create a comprehensive markdown report from aggregated response
        
        Returns path to the generated report
        """
        timestamp = datetime.fromtimestamp(response.timestamp)
        report_filename = f"report_{response.request_id}_{timestamp.strftime('%Y%m%d_%H%M%S')}.md"
        report_path = self.output_dir / report_filename
        
        # Generate report content
        report_content = self._generate_markdown_report(response)
        
        # Write to file
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"📄 Comprehensive report generated: {report_path}")
        return str(report_path)
    
    def _generate_markdown_report(self, response: AggregatedResponse) -> str:
        """Generate markdown content for the report"""
        timestamp = datetime.fromtimestamp(response.timestamp)
        
        lines = [
            f"# Samay Multi-Agent Query Report",
            f"",
            f"**Request ID:** `{response.request_id}`  ",
            f"**Timestamp:** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}  ",
            f"**Total Execution Time:** {response.total_execution_time:.2f} seconds  ",
            f"**Success Rate:** {response.successful_services}/{response.successful_services + response.failed_services} services  ",
            f"",
            f"## 📝 Original Prompt",
            f"",
            f"> {response.prompt}",
            f"",
            f"## 📊 Execution Summary",
            f"",
            f"| Service | Status | Time (s) | Retries |",
            f"|---------|--------|----------|---------|",
        ]
        
        # Add service summary table
        for resp in response.responses:
            status = "✅ Success" if resp.success else "❌ Failed"
            lines.append(f"| {resp.service.title()} | {status} | {resp.execution_time:.2f} | {resp.retry_count} |")
        
        lines.extend([
            f"",
            f"## 🤖 AI Service Responses",
            f""
        ])
        
        # Add individual responses
        for resp in response.responses:
            lines.extend(self._format_service_response(resp))
        
        # Add analysis section if multiple successful responses
        successful_responses = [r for r in response.responses if r.success]
        if len(successful_responses) > 1:
            lines.extend([
                f"## 🔍 Comparative Analysis",
                f"",
                self._generate_comparative_analysis(successful_responses),
                f""
            ])
        
        # Add metadata
        lines.extend([
            f"## 📋 Technical Details",
            f"",
            f"- **Total Services Queried:** {len(response.responses)}",
            f"- **Successful Responses:** {response.successful_services}",
            f"- **Failed Responses:** {response.failed_services}",
            f"- **Average Response Time:** {sum(r.execution_time for r in response.responses) / len(response.responses):.2f}s",
            f"- **Fastest Response:** {min(r.execution_time for r in response.responses):.2f}s ({min(response.responses, key=lambda x: x.execution_time).service})",
            f"- **Slowest Response:** {max(r.execution_time for r in response.responses):.2f}s ({max(response.responses, key=lambda x: x.execution_time).service})",
            f"",
            f"---",
            f"*Generated by Samay v3 Multi-Agent Assistant*"
        ])
        
        return "\n".join(lines)
    
    def _format_service_response(self, response: ServiceResponse) -> List[str]:
        """Format a single service response for the report"""
        lines = [
            f"### {response.service.title()}",
            f"",
            f"**Status:** {'✅ Success' if response.success else '❌ Failed'}  ",
            f"**Execution Time:** {response.execution_time:.2f} seconds  ",
            f"**Retry Count:** {response.retry_count}  ",
            f""
        ]
        
        if response.success:
            lines.extend([
                f"**Response:**",
                f"",
                f"```",
                response.response,
                f"```",
                f""
            ])
        else:
            lines.extend([
                f"**Error:**",
                f"",
                f"```",
                response.error_message,
                f"```",
                f""
            ])
        
        return lines
    
    def _generate_comparative_analysis(self, successful_responses: List[ServiceResponse]) -> str:
        """Generate comparative analysis of successful responses"""
        if len(successful_responses) < 2:
            return "Not enough successful responses for comparison."
        
        lines = [
            f"### Response Comparison",
            f"",
            f"**Services Compared:** {', '.join(r.service.title() for r in successful_responses)}",
            f"",
        ]
        
        # Simple analysis - in a real implementation, you might use NLP
        response_lengths = [(r.service, len(r.response.split())) for r in successful_responses]
        avg_length = sum(length for _, length in response_lengths) / len(response_lengths)
        
        lines.extend([
            f"**Response Characteristics:**",
            f"",
            f"- **Average Response Length:** {avg_length:.0f} words",
            f"- **Response Length Variation:**",
        ])
        
        for service, length in response_lengths:
            deviation = ((length - avg_length) / avg_length) * 100
            lines.append(f"  - {service.title()}: {length} words ({deviation:+.1f}% vs average)")
        
        lines.extend([
            f"",
            f"**Key Observations:**",
            f"",
            f"- All services provided successful responses",
            f"- Response times varied from {min(r.execution_time for r in successful_responses):.2f}s to {max(r.execution_time for r in successful_responses):.2f}s",
            f"- Consider comparing specific content themes for deeper analysis",
            f""
        ])
        
        return "\n".join(lines)
    
    def create_json_summary(self, response: AggregatedResponse) -> str:
        """Create a JSON summary for programmatic use"""
        timestamp = datetime.fromtimestamp(response.timestamp)
        summary_filename = f"summary_{response.request_id}_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        summary_path = self.output_dir / summary_filename
        
        summary = {
            "request_id": response.request_id,
            "timestamp": timestamp.isoformat(),
            "prompt": response.prompt,
            "execution_summary": {
                "total_time": response.total_execution_time,
                "successful_services": response.successful_services,
                "failed_services": response.failed_services,
                "success_rate": response.successful_services / (response.successful_services + response.failed_services)
            },
            "service_results": [
                {
                    "service": resp.service,
                    "success": resp.success,
                    "execution_time": resp.execution_time,
                    "retry_count": resp.retry_count,
                    "response_length": len(resp.response) if resp.success else 0,
                    "has_error": not resp.success,
                    "error_type": resp.error_message.split(':')[0] if not resp.success and ':' in resp.error_message else None
                }
                for resp in response.responses
            ],
            "performance_metrics": {
                "fastest_service": min(response.responses, key=lambda x: x.execution_time).service,
                "slowest_service": max(response.responses, key=lambda x: x.execution_time).service,
                "average_response_time": sum(r.execution_time for r in response.responses) / len(response.responses),
                "total_retries": sum(r.retry_count for r in response.responses)
            }
        }
        
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"📊 JSON summary generated: {summary_path}")
        return str(summary_path)
    
    def generate_full_report_suite(self, response: AggregatedResponse) -> Dict[str, str]:
        """Generate both markdown report and JSON summary"""
        return {
            "markdown_report": self.create_comprehensive_report(response),
            "json_summary": self.create_json_summary(response),
            "timestamp": datetime.fromtimestamp(response.timestamp).isoformat()
        }


def main():
    """Test the response aggregator"""
    from .prompt_dispatcher import PromptDispatcher, PromptRequest
    
    print("🚀 Samay v3 - Response Aggregator Test")
    print("=" * 50)
    
    # Create test aggregator
    aggregator = ResponseAggregator()
    
    # Test with sample data (you would normally get this from prompt_dispatcher)
    print("📝 This would typically use real responses from the prompt dispatcher")
    print("🔧 Run the main samay.py script to test with real multi-agent queries")
    
    print(f"📁 Reports will be saved to: {aggregator.output_dir}")


if __name__ == "__main__":
    main()